{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":205183965,"sourceType":"kernelVersion"},{"sourceId":139552,"sourceType":"modelInstanceVersion","modelInstanceId":118183,"modelId":127417}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Load 72B AWQ Model using vLLM on L4 x4\n\nIn this notebook, we load the awq quantized of Qwen/Qwen2.5-72B-Instruct. The model card on huggingface.com can be found [here](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct-AWQ).\n\n### Next steps\n\nSpecialized math models exist in the Qwen2.5 family of models. A good starting point might be to awq quantize their best math model: [Qwen/Qwen2.5-Math-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Math-72B-Instruct)!\n\nThe sky's the limit, happy kaggling!","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport ctypes\nimport warnings\n\nimport torch\nfrom vllm import LLM, SamplingParams","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:23:41.276811Z","iopub.execute_input":"2024-11-09T21:23:41.277338Z","iopub.status.idle":"2024-11-09T21:24:01.480744Z","shell.execute_reply.started":"2024-11-09T21:23:41.277292Z","shell.execute_reply":"2024-11-09T21:24:01.479957Z"}},"outputs":[{"name":"stderr","text":"2024-11-09 21:23:59,204\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"warnings.simplefilter('ignore')\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef clean_memory(deep=False):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:24:01.481774Z","iopub.execute_input":"2024-11-09T21:24:01.482371Z","iopub.status.idle":"2024-11-09T21:24:01.487012Z","shell.execute_reply.started":"2024-11-09T21:24:01.482337Z","shell.execute_reply":"2024-11-09T21:24:01.486236Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"llm_model_pth = '/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:24:01.488044Z","iopub.execute_input":"2024-11-09T21:24:01.488547Z","iopub.status.idle":"2024-11-09T21:24:01.498461Z","shell.execute_reply.started":"2024-11-09T21:24:01.488515Z","shell.execute_reply":"2024-11-09T21:24:01.497808Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"llm = LLM(\n    llm_model_pth,\n    dtype=\"half\",                # The data type for the model weights and activations\n    max_num_seqs=8,              # Maximum number of sequences per iteration. Default is 256\n    max_model_len=4096,          # Model context length\n    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,      # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.98, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:24:01.499304Z","iopub.execute_input":"2024-11-09T21:24:01.499550Z","iopub.status.idle":"2024-11-09T21:28:55.066648Z","shell.execute_reply.started":"2024-11-09T21:24:01.499523Z","shell.execute_reply":"2024-11-09T21:28:55.065772Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"INFO 11-09 21:24:21 awq_marlin.py:97] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 11-09 21:24:21 config.py:905] Defaulting to use mp for distributed inference\nINFO 11-09 21:24:21 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\nWARNING 11-09 21:24:22 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 11-09 21:24:22 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m INFO 11-09 21:24:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 11-09 21:24:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 11-09 21:24:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 11-09 21:24:23 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 11-09 21:24:23 pynccl.py:63] vLLM is using nccl==2.20.5\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:24:23 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 11-09 21:24:23 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 11-09 21:24:23 utils.py:1008] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:24:23 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 11-09 21:24:23 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 11-09 21:24:23 pynccl.py:63] vLLM is using nccl==2.20.5\nWARNING 11-09 21:24:24 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m WARNING 11-09 21:24:24 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 11-09 21:24:24 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 11-09 21:24:24 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 11-09 21:24:24 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x782145c30a60>, local_subscribe_port=57611, remote_subscribe_port=None)\nINFO 11-09 21:24:24 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:24:24 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\nINFO 11-09 21:24:24 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\nINFO 11-09 21:24:24 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9684ffdace304ac88f689308c576833b"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:28:39 model_runner.py:1067] Loading model weights took 9.7875 GB\nINFO 11-09 21:28:39 model_runner.py:1067] Loading model weights took 9.7875 GB\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m INFO 11-09 21:28:39 model_runner.py:1067] Loading model weights took 9.7875 GB\n\u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m INFO 11-09 21:28:39 model_runner.py:1067] Loading model weights took 9.7875 GB\nINFO 11-09 21:28:46 distributed_gpu_executor.py:57] # GPU blocks: 9178, # CPU blocks: 3276\nINFO 11-09 21:28:46 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 35.85x\nINFO 11-09 21:28:49 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 11-09 21:28:49 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m INFO 11-09 21:28:49 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n\u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:28:49 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n\u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m INFO 11-09 21:28:49 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n\u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m INFO 11-09 21:28:49 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 11-09 21:28:49 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m INFO 11-09 21:28:49 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 11-09 21:28:54 model_runner.py:1523] Graph capturing finished in 5 secs.\n\u001b[1;36m(VllmWorkerProcess pid=1101)\u001b[0;0m INFO 11-09 21:28:54 model_runner.py:1523] Graph capturing finished in 5 secs.\n\u001b[1;36m(VllmWorkerProcess pid=1103)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1102)\u001b[0;0m INFO 11-09 21:28:54 model_runner.py:1523] Graph capturing finished in 5 secs.\nINFO 11-09 21:28:54 model_runner.py:1523] Graph capturing finished in 5 secs.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"sampling_params = SamplingParams(\n    temperature=0.3,              # randomness of the sampling\n    seed=1,                       # Seed for reprodicibility\n    skip_special_tokens=False,\n    max_tokens=2400\n)\n\nmsgs = [\n    {\"role\": \"user\", \"content\": \"give me a step-by-step explanation of the intermediate value theorem\"}\n]\n\nresponse = llm.chat(msgs, sampling_params, use_tqdm=False)\n\nprint(response[0].outputs[0].text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:32:18.207477Z","iopub.execute_input":"2024-11-09T21:32:18.208321Z","iopub.status.idle":"2024-11-09T21:32:58.643656Z","shell.execute_reply.started":"2024-11-09T21:32:18.208279Z","shell.execute_reply":"2024-11-09T21:32:58.642918Z"}},"outputs":[{"name":"stdout","text":"Certainly! The Intermediate Value Theorem (IVT) is a fundamental theorem in calculus that provides a way to determine if a function takes on a certain value within a given interval. Here’s a step-by-step explanation:\n\n### Step 1: Understand the Theorem\nThe Intermediate Value Theorem states that if a function \\( f \\) is continuous on a closed interval \\([a, b]\\) and \\( N \\) is any number between \\( f(a) \\) and \\( f(b) \\), then there exists at least one number \\( c \\) in the interval \\((a, b)\\) such that \\( f(c) = N \\).\n\n### Step 2: Define the Conditions\n1. **Continuity**: The function \\( f \\) must be continuous on the closed interval \\([a, b]\\). This means that there are no breaks, jumps, or holes in the function within this interval.\n2. **Closed Interval**: The interval \\([a, b]\\) is closed, meaning it includes both endpoints \\( a \\) and \\( b \\).\n3. **Value \\( N \\)**: \\( N \\) is any number between \\( f(a) \\) and \\( f(b) \\). This means \\( N \\) lies within the range of the function values at the endpoints.\n\n### Step 3: Visualize the Theorem\nImagine a continuous curve that starts at \\( (a, f(a)) \\) and ends at \\( (b, f(b)) \\). If you draw a horizontal line at \\( y = N \\) (where \\( N \\) is between \\( f(a) \\) and \\( f(b) \\)), the curve must cross this line at least once.\n\n### Step 4: Apply the Theorem\n1. **Check Continuity**: Verify that \\( f \\) is continuous on \\([a, b]\\).\n2. **Identify \\( f(a) \\) and \\( f(b) \\)**: Calculate the values of the function at the endpoints \\( a \\) and \\( b \\).\n3. **Choose \\( N \\)**: Select a number \\( N \\) such that \\( f(a) \\leq N \\leq f(b) \\) or \\( f(b) \\leq N \\leq f(a) \\).\n4. **Find \\( c \\)**: The theorem guarantees that there is at least one number \\( c \\) in the interval \\((a, b)\\) such that \\( f(c) = N \\).\n\n### Step 5: Example\nLet's consider a specific example to illustrate the theorem.\n\n**Example:**\n- Let \\( f(x) = x^2 \\) on the interval \\([1, 3]\\).\n- Calculate \\( f(1) \\) and \\( f(3) \\):\n  \\[\n  f(1) = 1^2 = 1\n  \\]\n  \\[\n  f(3) = 3^2 = 9\n  \\]\n- Choose \\( N = 5 \\), which is between 1 and 9.\n- According to the IVT, there exists at least one \\( c \\) in the interval \\((1, 3)\\) such that \\( f(c) = 5 \\).\n\n**Solve for \\( c \\):**\n\\[\nc^2 = 5 \\implies c = \\sqrt{5}\n\\]\nSince \\( \\sqrt{5} \\approx 2.236 \\) lies within the interval \\((1, 3)\\), the theorem is satisfied.\n\n### Step 6: Conclusion\nThe Intermediate Value Theorem is a powerful tool for proving the existence of solutions to equations and for understanding the behavior of continuous functions. It guarantees that if a function is continuous on a closed interval and takes on values at the endpoints, it must take on every value between those endpoints at least once within the interval.\n\nI hope this step-by-step explanation helps you understand the Intermediate Value Theorem!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"sampling_params = SamplingParams(\n    temperature=0.3,              # randomness of the sampling\n    seed=1,                       # Seed for reprodicibility\n    skip_special_tokens=False,\n    max_tokens=2400\n)\n\nmsgs = [\n    {\"role\": \"user\", \"content\": \"what are the different steps involved in implementing Godel's theorem in python? Note that (1) I already have an array which associates each logical symbol with an integer, and (2) I already have a decoding function to pass from an integer to a formula, using parsing.\"}\n]\n\nresponse = llm.chat(msgs, sampling_params, use_tqdm=False)\n\nprint(response[0].outputs[0].text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:39:03.819874Z","iopub.execute_input":"2024-11-09T21:39:03.820587Z","iopub.status.idle":"2024-11-09T21:40:11.909710Z","shell.execute_reply.started":"2024-11-09T21:39:03.820548Z","shell.execute_reply":"2024-11-09T21:40:11.908940Z"}},"outputs":[{"name":"stdout","text":"Implementing Gödel's theorem in Python involves several steps, primarily focusing on encoding and decoding logical statements, and then demonstrating the key aspects of the theorem, such as the existence of undecidable statements. Given that you already have an array associating each logical symbol with an integer and a decoding function, we can proceed with the following steps:\n\n### Step 1: Define the Encoding Function\nYou need a function to encode a logical formula into a Gödel number. This function will use the array you provided to map each symbol to an integer and then combine these integers into a single number.\n\n```python\ndef encode_formula(formula, symbol_to_int):\n    \"\"\"\n    Encode a logical formula into a Gödel number.\n    \n    :param formula: A string representing the logical formula.\n    :param symbol_to_int: A dictionary mapping logical symbols to integers.\n    :return: The Gödel number of the formula.\n    \"\"\"\n    # Split the formula into individual symbols\n    symbols = list(formula)\n    \n    # Map each symbol to its corresponding integer\n    integers = [symbol_to_int[symbol] for symbol in symbols]\n    \n    # Combine the integers into a single Gödel number\n    # For simplicity, we can use a product of prime numbers raised to the power of the integers\n    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n    godel_number = 1\n    for i, integer in enumerate(integers):\n        godel_number *= primes[i] ** integer\n    \n    return godel_number\n```\n\n### Step 2: Define the Decoding Function\nYou already have a decoding function, but for completeness, here is an example:\n\n```python\ndef decode_godel_number(godel_number, int_to_symbol):\n    \"\"\"\n    Decode a Gödel number back into a logical formula.\n    \n    :param godel_number: The Gödel number to decode.\n    :param int_to_symbol: A dictionary mapping integers to logical symbols.\n    :return: The logical formula as a string.\n    \"\"\"\n    # Factorize the Gödel number into its prime factors\n    factors = []\n    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n    for prime in primes:\n        if godel_number == 1:\n            break\n        count = 0\n        while godel_number % prime == 0:\n            godel_number //= prime\n            count += 1\n        if count > 0:\n            factors.append((prime, count))\n    \n    # Map the factors back to symbols\n    symbols = [int_to_symbol[count] for _, count in factors]\n    \n    # Combine the symbols into a formula\n    formula = ''.join(symbols)\n    \n    return formula\n```\n\n### Step 3: Define a Function to Generate Gödel Numbers for Statements\nYou need a function to generate Gödel numbers for specific logical statements, such as the Gödel sentence.\n\n```python\ndef generate_godel_sentence(symbol_to_int):\n    \"\"\"\n    Generate a Gödel sentence and its Gödel number.\n    \n    :param symbol_to_int: A dictionary mapping logical symbols to integers.\n    :return: The Gödel sentence and its Gödel number.\n    \"\"\"\n    # Define a simple Gödel sentence (for demonstration purposes)\n    godel_sentence = \"P(0) -> P(1)\"\n    \n    # Encode the Gödel sentence\n    godel_number = encode_formula(godel_sentence, symbol_to_int)\n    \n    return godel_sentence, godel_number\n```\n\n### Step 4: Demonstrate the Undecidability\nTo demonstrate the undecidability aspect of Gödel's theorem, you can show that the Gödel sentence is true but not provable within the system.\n\n```python\ndef demonstrate_undecidability(godel_sentence, godel_number, int_to_symbol):\n    \"\"\"\n    Demonstrate the undecidability of the Gödel sentence.\n    \n    :param godel_sentence: The Gödel sentence.\n    :param godel_number: The Gödel number of the Gödel sentence.\n    :param int_to_symbol: A dictionary mapping integers to logical symbols.\n    :return: None\n    \"\"\"\n    print(f\"Gödel Sentence: {godel_sentence}\")\n    print(f\"Gödel Number: {godel_number}\")\n    \n    # Decode the Gödel number to verify the original formula\n    decoded_formula = decode_godel_number(godel_number, int_to_symbol)\n    print(f\"Decoded Formula: {decoded_formula}\")\n    \n    # Demonstrate that the Gödel sentence is true but not provable\n    print(\"The Gödel sentence is true but not provable within the system.\")\n```\n\n### Step 5: Run the Demonstration\nFinally, run the demonstration to show the undecidability of the Gödel sentence.\n\n```python\n# Example symbol-to-integer mapping\nsymbol_to_int = {\n    'P': 1,\n    '0': 2,\n    '1': 3,\n    '->': 4,\n    '(': 5,\n    ')': 6\n}\n\n# Invert the mapping for decoding\nint_to_symbol = {v: k for k, v in symbol_to_int.items()}\n\n# Generate the Gödel sentence and its Gödel number\ngodel_sentence, godel_number = generate_godel_sentence(symbol_to_int)\n\n# Demonstrate the undecidability\ndemonstrate_undecidability(godel_sentence, godel_number, int_to_symbol)\n```\n\n### Explanation\n1. **Encoding Function**: Converts a logical formula into a Gödel number using a product of prime numbers.\n2. **Decoding Function**: Converts a Gödel number back into a logical formula.\n3. **Generate Gödel Sentence**: Creates a simple Gödel sentence and encodes it.\n4. **Demonstrate Undecidability**: Shows that the Gödel sentence is true but not provable within the system.\n\nThis implementation provides a basic framework for understanding and demonstrating Gödel's theorem in Python. For a more rigorous and complete implementation, you would need to handle more complex logical systems and proofs.\n","output_type":"stream"}],"execution_count":11}]}
